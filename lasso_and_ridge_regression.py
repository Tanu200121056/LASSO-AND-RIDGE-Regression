# -*- coding: utf-8 -*-
"""LASSO AND RIDGE REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKkpNeafM2Vifw3WW98AbJZHBteMnVQL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

pip install sidetable --upgrade

import sidetable

pip install feature_engine

### Importing necessary libraries
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from feature_engine.outliers import Winsorizer

from sklearn.linear_model import LinearRegression
from statsmodels.tools.tools import add_constant

from sklearn.metrics import r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.model_selection import train_test_split
import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn.model_selection import GridSearchCV

import joblib
import pickle

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Assuming you uploaded a CSV file
df = pd.read_csv('50_Startups.csv')

#### Descriptive Statistics and Data Distribution
df.describe()

# Missing values check
df.isnull().any()
df.info()

# Seperating input and output variables
X = pd.DataFrame(df.iloc[:, 0:4])
y = pd.DataFrame(df.iloc[:, 4])

X.head()

y.head()

# Checking for unique values
X["State"].unique()
X["State"].value_counts()

# Build a frequency table using sidetable library
X.stb.freq(["State"])

# Segregating Non-Numeric features
categorical_features = X.select_dtypes(include = ['object']).columns
print(categorical_features)

# Segregating Numeric features
numeric_features = X.select_dtypes(exclude = ['object']).columns
print(numeric_features)

## Missing values Analysis
# Define pipeline for missing data if any

num_pipeline = Pipeline(steps = [('impute', SimpleImputer(strategy = 'mean'))])

preprocessor = ColumnTransformer(transformers = [('num', num_pipeline, numeric_features)])

# Fit the imputation pipeline to input features
imputation = preprocessor.fit(X)

# Save the pipeline
joblib.dump(imputation, 'meanimpute')

# Transformed data
cleandata = pd.DataFrame(imputation.transform(X), columns = numeric_features)
cleandata.head()

# pandas plot() function with parameters kind = 'box' and subplots = True

X.plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18))

'''sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''

# Increase spacing between subplots
plt.subplots_adjust(wspace = 0.75) # ws is the width of the padding between subplots, as a fraction of the average Axes width.
plt.show()

# Winsorization for outlier treatment
winsor = Winsorizer(capping_method = 'iqr', # choose  IQR rule boundaries or gaussian for mean and std
                          tail = 'both', # cap left, right or both tails
                          fold = 1.5,
                          variables = list(cleandata.columns))

winsor

clean = winsor.fit(cleandata)

# Save winsorizer model
joblib.dump(clean, 'winsor')

cleandata1 = pd.DataFrame(clean.transform(cleandata), columns = numeric_features)

# Boxplot
cleandata1.plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18))
plt.subplots_adjust(wspace = 0.75) # ws is the width of the padding between subplots, as a fraction of the average Axes width.
plt.show()

# Scaling
## Scaling with MinMaxScaler
scale_pipeline = Pipeline([('scale', MinMaxScaler())])

scale_columntransfer = ColumnTransformer([('scale', scale_pipeline, numeric_features)]) # Skips the transformations for remaining columns

scale = scale_columntransfer.fit(cleandata1)

# Save Minmax scaler pipeline model
joblib.dump(scale, 'minmax')

scaled_data = pd.DataFrame(scale.transform(cleandata1), columns = numeric_features)
scaled_data.describe()

## Encoding
# Categorical features
encoding_pipeline = Pipeline([('onehot', OneHotEncoder())])

preprocess_pipeline = ColumnTransformer([('categorical', encoding_pipeline, categorical_features)])

clean =  preprocess_pipeline.fit(X)   # Works with categorical features only

# Save the encoding model
joblib.dump(clean, 'encoding')

# Transform the data using the fitted pipeline
encoded_data = clean.transform(X)

# Convert the encoded data to a DataFrame
encode_data = pd.DataFrame(encoded_data, columns=clean.get_feature_names_out(input_features=X.columns))

# Display information about the encoded DataFrame
encode_data.info()

# Assuming scaled_data is already defined and corresponds to your numerical features
# Concatenate the numerical features and the encoded categorical features
clean_data = pd.concat([scaled_data, encode_data], axis=1)

# Display information about the concatenated DataFrame
clean_data.info()
clean_data.describe().T  # Transposed view

# Library to call OLS model
# import statsmodels.api as sm

# Build a vanilla model on full dataset

# By default, statsmodels fits a line passing through the origin, i.e. it
# doesn't fit an intercept. Hence, you need to use the command 'add_constant'
# so that it also fits an intercept

P = add_constant(clean_data)
basemodel = sm.OLS(y, P).fit()
basemodel.summary()

# p-values of coefficients found to be insignificant due to colinearity

# Identify the variale with highest colinearity using Variance Inflation factor (VIF)
# Variance Inflation Factor (VIF)
# Assumption: VIF > 10 = colinearity
# VIF on clean Data
vif = pd.Series([variance_inflation_factor(P.values, i) for i in range(P.shape[1])], index = P.columns)
vif
# inf = infinity

# Tune the model by verifying for influential observations
sm.graphics.influence_plot(basemodel)

"""# Regularization Techniques: LASSO, RIDGE and ElasticNet Regression

# **LASSO Regession Model**
"""

from sklearn.linear_model import Lasso
#help(Lasso)

lasso = Lasso(alpha = 0.13)

lasso.fit(clean_data, y)

# Coefficient values for all independent variables#
lasso.intercept_

lasso.coef_

# Create the bar plot
plt.figure(figsize=(12, 6))  # Adjust the figure size as needed
plt.bar(x=pd.Series(clean_data.columns), height=pd.Series(lasso.coef_))

# Rotate the x labels for better readability
plt.xticks(rotation=45, ha='right')

# Add labels and title
plt.xlabel('Features')
plt.ylabel('Coefficient Values')
plt.title('Lasso Regression Coefficients')

# Add grid lines for better readability
plt.grid(axis='y')

# Show the plot
plt.tight_layout()  # Adjust layout to make room for rotated labels
plt.show()

# Create a function called lasso,
pred_lasso = lasso.predict(clean_data)

# Adjusted r-square
s1 = lasso.score(clean_data, y['Profit'])
s1

# RMSE
np.sqrt(np.mean((pred_lasso - np.array(y['Profit']))**2))

"""# **RIDGE Regression Model**"""

from sklearn.linear_model import Ridge


rm = Ridge(alpha = 0.13)

rm.fit(clean_data, y)

# Coefficients values for all the independent vairbales
rm.intercept_

rm.coef_

result = rm.coef_.flatten()
result

# Create the bar plot
plt.figure(figsize=(12, 6))  # Adjust the figure size as needed
plt.bar(x=pd.Series(clean_data.columns), height=pd.Series(result))

# Rotate the x labels for better readability
plt.xticks(rotation=45, ha='right')

# Add labels and title
plt.xlabel('Features')
plt.ylabel('Coefficient Values')
plt.title('Ridge Regression Coefficients')

# Add grid lines for better readability
plt.grid(axis='y')

# Show the plot
plt.tight_layout()  # Adjust layout to make room for rotated labels
plt.show()

rm.alpha

pred_rm = rm.predict(clean_data)

# Adjusted r-square
s2 = rm.score(clean_data, y['Profit'])
s2

# RMSE
np.sqrt(np.mean((pred_rm - np.array(y['Profit']))**2))

"""# **ELASTIC NET Regression Model**"""

from sklearn.linear_model import ElasticNet


enet = ElasticNet(alpha = 0.13)

enet.fit(clean_data, y['Profit'])

# Coefficients values for all the independent vairbales
enet.intercept_

enet.coef_

# Create the bar plot
plt.figure(figsize=(12, 6))  # Adjust the figure size as needed
plt.bar(x=pd.Series(clean_data.columns), height=pd.Series(enet.coef_))

# Rotate the x labels for better readability
plt.xticks(rotation=45, ha='right')

# Add labels and title
plt.xlabel('Features')
plt.ylabel('Coefficient Values')
plt.title('Elastic Net Regression Coefficients')

# Add grid lines for better readability
plt.grid(axis='y')

# Show the plot
plt.tight_layout()  # Adjust layout to make room for rotated labels
plt.show()

pred_enet = enet.predict(clean_data)

# Adjusted r-square
s3 = enet.score(clean_data, y['Profit'])
s3

# RMSE
np.sqrt(np.mean((pred_enet - np.array(y['Profit']))**2))

##########################################################################################
# Lasso Regression
# from sklearn.model_selection import GridSearchCV

parameters = {'alpha': [1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.13, 0.2, 1, 5, 10, 20]}

lasso = Lasso()

lasso_reg = GridSearchCV(lasso, parameters, scoring = 'r2', cv = 5)
lasso_reg.fit(clean_data, y['Profit'])

lasso_reg.best_params_

lasso_reg.best_score_

lasso_pred = lasso_reg.predict(clean_data)

# Adjusted r-square#
s4 = lasso_reg.score(clean_data, y['Profit'])
s4

# RMSE
np.sqrt(np.mean((lasso_pred - np.array(y['Profit']))**2))

# Ridge Regression
# from sklearn.model_selection import GridSearchCV
# from sklearn.linear_model import Ridge

ridge = Ridge()

# parameters = {'alpha': [1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.13, 0.2, 1, 5, 10, 20]}

ridge_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = 5)
ridge_reg.fit(clean_data, y['Profit'])

ridge_reg.best_params_

ridge_reg.best_score_

ridge_pred = ridge_reg.predict(clean_data)

# Adjusted r-square#
s5 = ridge_reg.score(clean_data, y['Profit'])
s5

# RMSE
np.sqrt(np.mean((ridge_pred - np.array(y['Profit']))**2))

# ElasticNet Regression
# from sklearn.model_selection import GridSearchCV
# from sklearn.linear_model import ElasticNet

enet = ElasticNet()

# parameters = {'alpha': [1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.13, 0.2, 1, 5, 10, 20]}

enet_reg = GridSearchCV(enet, parameters, scoring = 'r2', cv = 5)

enet_reg.fit(clean_data, y['Profit'])

enet_reg.best_params_

enet_reg.best_score_

enet_pred = enet_reg.predict(clean_data)

# Adjusted r-square
s6 = enet_reg.score(clean_data, y['Profit'])
s6

# RMSE
np.sqrt(np.mean((enet_pred - np.array(y['Profit']))**2))

scores_all = pd.DataFrame({'models':['Lasso', 'Ridge', 'Elasticnet', 'Grid_lasso', 'Grid_ridge', 'Grid_elasticnet'], 'Scores':[s1, s2, s3, s4, s5, s6]})
scores_all

# Save the Best score model
finalgrid = lasso_reg.best_estimator_
finalgrid

# Save the best model
pickle.dump(finalgrid, open('grid_best.pkl', 'wb'))

##########