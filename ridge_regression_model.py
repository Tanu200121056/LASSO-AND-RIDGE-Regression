# -*- coding: utf-8 -*-
"""RIDGE Regression Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157qnliPX-aSMaPCPPVM18Hn4Tp-zevsZ

# RIDGE Regression Model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import sidetable

pip install feature_engine

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder
from feature_engine.outliers import Winsorizer

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

from statsmodels.tools.tools import add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.formula.api as smf
import statsmodels.api as sm

import joblib
import pickle

from google.colab import files
uploaded = files.upload()

# Assuming you uploaded a CSV file
df = pd.read_csv('50_Startups (1).csv')

df.describe()

df.info()

df.isnull().any()

X = pd.DataFrame(df.iloc[: , 0:4])
X

y = pd.DataFrame(df.iloc[:,4])
y

X['State'].unique()

X['State'].value_counts()

# Build a frequency table using sidetable library

X.stb.freq(["State"])

# Segregating Non-Numeric features

categorical_features = X.select_dtypes(include = 'object').columns
print(categorical_features)

# Segregating Numeric features

numeric_features = X.select_dtypes(exclude = 'object').columns
print(numeric_features)

## Missing values Analysis
# Define pipeline for missing data if any


num_pipeline = Pipeline(steps = [('impute', SimpleImputer(strategy = 'mean'))])

preprocessor = ColumnTransformer(transformers = [('num', num_pipeline, numeric_features)])

# Fit the imputation pipeline to input features

imputation = preprocessor.fit(X)

# Save the pipeline

joblib.dump(imputation, 'meanimpute')

# Transformed data

cleandata = pd.DataFrame(imputation.transform(X), columns = numeric_features)
cleandata.head()

# pandas plot() function with parameters kind = 'box' and subplots = True

X.plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18))

'''sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''

# Increase spacing between subplots
plt.subplots_adjust(wspace = 0.75) # ws is the width of the padding between subplots, as a fraction of the average Axes width.
plt.show()

# Winsorization for outlier treatment

winsor = Winsorizer(capping_method = 'iqr', # choose  IQR rule boundaries or gaussian for mean and std
                          tail = 'both',    # cap left, right or both tails
                          fold = 1.5,
                          variables = list(cleandata.columns))

winsor

clean = winsor.fit(cleandata)

# Save winsorizer model
joblib.dump(clean, 'winsor')

cleandata1 = pd.DataFrame(clean.transform(cleandata), columns = numeric_features)

# Boxplot

cleandata1.plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18))
plt.subplots_adjust(wspace = 0.75) # ws is the width of the padding between subplots, as a fraction of the average Axes width.
plt.show()

# Scaling
## Scaling with MinMaxScaler

scale_pipeline = Pipeline([('scale', MinMaxScaler())])

scale_columntransfer = ColumnTransformer([('scale', scale_pipeline, numeric_features)]) # Skips the transformations for remaining columns

scale = scale_columntransfer.fit(cleandata1)

# Save Minmax scaler pipeline model
joblib.dump(scale, 'minmax')

scaled_data = pd.DataFrame(scale.transform(cleandata1), columns = numeric_features)
scaled_data.describe()

## Encoding
# Categorical features

encoding_pipeline = Pipeline([('onehot', OneHotEncoder())])

preprocess_pipeline = ColumnTransformer([('categorical', encoding_pipeline, categorical_features)])

clean =  preprocess_pipeline.fit(X)   # Works with categorical features only

# Save the encoding model

joblib.dump(clean, 'encoding')

# Transform the data using the fitted pipeline

encoded_data = clean.transform(X)

# Convert the encoded data to a DataFrame

encode_data = pd.DataFrame(encoded_data, columns=clean.get_feature_names_out(input_features=X.columns))

# Display information about the encoded DataFrame

encode_data.info()

# Assuming scaled_data is already defined and corresponds to your numerical features
# Concatenate the numerical features and the encoded categorical features

clean_data = pd.concat([scaled_data, encode_data], axis=1)

# Display information about the concatenated DataFrame

clean_data.info()
clean_data.describe().T  # Transposed view

# Library to call OLS model
# import statsmodels.api as sm

# Build a vanilla model on full dataset

# By default, statsmodels fits a line passing through the origin, i.e. it
# doesn't fit an intercept. Hence, you need to use the command 'add_constant'
# so that it also fits an intercept

P = add_constant(clean_data)
basemodel = sm.OLS(y, P).fit()
basemodel.summary()

# p-values of coefficients found to be insignificant due to colinearity

# Identify the variale with highest colinearity using Variance Inflation factor (VIF)
# Variance Inflation Factor (VIF)
# Assumption: VIF > 10 = colinearity
# VIF on clean Data

vif = pd.Series([variance_inflation_factor(P.values, i) for i in range(P.shape[1])], index = P.columns)
vif

# inf = infinity

# Tune the model by verifying for influential observations

sm.graphics.influence_plot(basemodel)

from sklearn.linear_model import Ridge


rm = Ridge(alpha = 0.13)

rm.fit(clean_data, y)

# Coefficients values for all the independent vairbales
rm.intercept_

rm.coef_

result = rm.coef_.flatten()
result

# Create the bar plot
plt.figure(figsize=(12, 6))  # Adjust the figure size as needed
plt.bar(x=pd.Series(clean_data.columns), height=pd.Series(result))

# Rotate the x labels for better readability
plt.xticks(rotation=45, ha='right')

# Add labels and title
plt.xlabel('Features')
plt.ylabel('Coefficient Values')
plt.title('Ridge Regression Coefficients')

# Add grid lines for better readability
plt.grid(axis='y')

# Show the plot
plt.tight_layout()  # Adjust layout to make room for rotated labels
plt.show()

rm.alpha

pred_rm = rm.predict(clean_data)

# Adjusted r-square
s2 = rm.score(clean_data, y['Profit'])
s2

# RMSE
np.sqrt(np.mean((pred_rm - np.array(y['Profit']))**2))